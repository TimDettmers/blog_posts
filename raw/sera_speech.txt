So recording the blog post narration while making breakfast. So this blog post is about my journey towards coding agents. We have a very exciting release that represents state-of-the-art coding agents. while making breakfast. And they have the ability to rapidly specialize to any private data. And this is kind of the holy grail of coding agents. So many companies want to specialize codingagents to their own repositories, to their private code bases. And that has been a very difficult problem. And we build a method that solves that problem. But this block code is not only about that result, but also on the journey towards it. contains all the information that is often hidden, all the paths taken and actually failed. And taken together, this gives you a more holistic perspective on what it was like to work on this project, make progress, and also share the all the in-depth knowledge that goes beyond the papers, the knowledge. that is between the lines and between the words in the paper. which wasn't easy. And I will talk about all these things in turn. The first thing in the first section in the blog post is switching fields are scary. That is a headline. I've started in a very good position. I becamean expert in quantization. And through my expertise, could build methods like ULORA, K-bit inference scaling laws. These have been hugely successful, particularly when I integrated them in the Bits and Bites library. Bits and Bites reached millions of downloads per month. and Killora became a staple in the industry, And research means extending knowledge. And for me, it was clear that Shetbot models will not yield the productivity improvements that we need. topic. I started in August 2024 working on coding agents and it was scary. I just And that's probably one of the hardest things if you switch a field. People were very excited that I was joining Dallin Institute for AI. I brought a lot of expertise with me. But then, after I joined, I more or less said, I will not do anything that is related to my expertise. And I will learn something new. And that was pretty disappointing to many. we're excited that I might be able to help them with their problems. But that's SWEbench performance of the open weight model that I could easily use, an 8 billion So, since I'm a fan of open source, the proponent, I naturally worked with open weight models. And when I looked at the Sweebench performance of the open weight model that I couldeasily use, an 8 billion model, I saw that it's basically 0%. Sweebench, of course, is a benchmark. you get a good up issue in a code base, then you need to solve the bug that is presented in the good up issue. Now if you looked at closed source models at the time, they performed much much better. It was around 30% while the 8-Berry model had 0%. So my first challenge was that I set myself was, can I improve the of this 8 billion model as much as possible make open source competitive with run less experiments. Each experiment is more important. And that means you've looked very, very carefully at each trajectory that you run, if you have a model, and you also see what's out there. And that was my first step. I downloaded all the trajectories from some frontier models, and then I looked at them. I looked at two things. The workflow that models use, the solve bugs and then also problems they encounter. When analyzing the workflow, it was very quickly clear that each coding model of all the closed source coding models learn the same workflow. And this is a workflow that actually closely mirrors what humans do. They read the GitHub issue, try to understand where in the code base is related functions and classes. once they find that they go a little bit deeper they see how are these functions related to other things try to get an understanding of the problem then they home in on the problem and once found they work on a bug fix the next steps can be interchanged either you write first a bug fix and then a replication script or first a replication script and then a bug fix technically it is better to first write a replication script to see if the bug is really bug and then run the then create the bug fix but the closed models would sometimes interchange these steps then comes an iteration stage where the editing is iterated until the replication no longer throws an error and then you create a patch and submit it closed source models work on speedbench. and look at the trajectories to see what's going wrong. problem. The small open-weight models are very bad at copying information. Copying information is extremely important for tool calling. If you want to explore a grab, for example. These small imprecisions and tool calls was exactly what models, small open-weight models stopped from getting any solutions, a solve rate of 0%. One could easily solve this problem by doing a fuzzy search against existing functions. So their tool calls relating to a function are corrected. This immediately led to a 7% solve rate, a big improvement. From there it was easier to run trajectories and see closing tag that was required for the tool. And with that, no tool call was performed, which confused the model and led to errors. The simple fix was to make sure that tags are always closed for tool calls. This and similar fixes pushed the performance quickly to 14%. The big lesson was open weight models were not good at copying information where closed source models were really good at it. And so, equipped with his knowledge, I then proceeded to make a couple of further changes that made these smaller models more robust to refer to symbolic information. That improved performance significantly. And I could push the 7 billion model performance up to 24%, which was very close to the performance of GPT-40 that sits at 25%. All of this was enabled by estimating the variance in overall results. And that helped me to reliably measure if certain things improve results or not. This is something I learned from writing the QLR paper, running thousands of experiments that are small, helped to verify the variables that were important at the larger scale. This was a very exciting result. It made mefeel that I get a foothold very off-coding agents. This was in November 2024, and after these results, people, AI2, were excited. Many people were interested in applying many of the methods that are developed to their agent workflows. Nobody at AIR2 was working on coding agents, but people worked on scientific agents. And so at this point I stopped working on coding agents and tried to build a general agent framework that works for scientific agents and works more broadly beyond that. But at that time misfortune struck. I developed some health problems that forced me to take a break. And so while that was a difficult decision, I basically stopped working from February 2025. I still committed to hiring an intern, Edna Shen, who was very highly recommended from France and had an impressive So while it didn't work, I wanted to not leave Ethan hanging when he arrived and wanted to engine before, but together with my expertise, he made very rapid progress. Instead of improving not leave Ethan hanging when he arrived and wanted to mentor him. Ethan started one day before Cloud Code was released. He didn't work on coding engine before, but together with my expertise, he made very rapid progress. Instead of improving the scaffolds for open source models, this was also a time when Quent 3 was released. And together with the reasoning, The open source models now gain the precision to make true calls that are mostly correct. means realtraining data with real issues. So this meant data work was very difficult to do But it was very clear that that is not scalable for two reasons. Ethan and I was working alone while I was just mentoring himand not working myself on this problem. So this meant data work was very difficult to do with a single person that hasn't yet worked on coding agents. So, the only scalable solution was to generate synthetic data to then train on it. At that time, most people wanted to generate correct training data. For example, you look at GitHub issues with bugs that describe bugs in a code base, and that means you have thefinal solution, the patch, you have the faulty state, the bug, repository of the bug then you generate synthetic training data from a rollout simple approach the core problem is how to generate labels but one approach that's synthetic training data from a rollout, from a larger model that you can then use to fine-tune a smaller model. But if you want to work with synthetic data, you need to take a different approach. And there's one simple approach. The core problem is how to generate labels. But one approach that's very simple is to start with the label, then mutate your data to generate the example. you start with the correct codebase, then you generate a bug, and then you take the bug as initial example, and the correct codebase as the final state, the target state, the label. This makes it easy to generate synthetic data on Mars. Beyond this, we worked on multiple approaches that I discussed in the next section. And this is improving coding agents with data. There's a subtask splitting method, an end-to-end method,and then SARA, which allows the rapid specialization of any coding agent to a private code base. code search, search for a bug is one of the most important problems. If you can't find the bug, you can't solve it. And if you found the bug, you had a good chance at solving it. That means if you have a trajectory, then the most important thing to learn is searching the code base for a bug. And why not first learnthat, and then learn editing afterwards? This has also an advantage because if you divide a problem in subtasks, then methods for subtasks might be shared across different problems. For example, searching acode base might be useful in many different ways of working on coding problems or any kind of digital problem. Searching for information is a very general important task. another problem that's used across many other problems, for example adding a feature, fixing a bug and so forth. So by splitting into subtasks, we hope to gain efficiency. Again because we're resource limited, first doing well on search trajectories gave us the hope that we are then more efficient on the learning editing. because we can already find bugs with higher precision. If you want to do this there's one problem. You don't have any labelsfor subtasks, but here we can use a mutation method. We go from a correct state, then mutate the correct state into a problem state, and then we use the target, we use the initial state as a label in the mutated state as an input example. With this definition there are countless tasks that you can generate to help the model learn searching information in the cookbase. So how do you find which tasks is a good task to generate data from and to learn and there's a simple method that's very effective and what you do is to take multiple models, then you generate a subtask, a proxy task, through a mutation, then you evaluate different models on the subtask and on your target task suite bench. And then you compute the correlation. If a task is relevant, you will get a correlation close to one. That means the better you areon the subtask, the better you do on subtask suite bench. And if your proxy task is bad, the correlation will be low. That means even function in a code base, then have a language model summarize a functionas a text, and you create a task where the model gets the summary of a function and a code base, and then it needs to search the code base and stop once it thinks it has found the function that is describedin the summary. This task has almost a one-to-one correlation with SWIbench, which shows that search is very important for SWIbench, and that you quite easily can create a task that mirrors this behavior. If you fine-tune on just 500 samples generated from this task, small open source models, even 8 billion models, become almost as good as frontier models as searching information and code bases. This heavily boosts performance for open-weight models, particularly for small models. You very easily reach state-of-the-art results already with this method. But we wanted to push it further. In this split task design, you have search and editing. Well, the second step was editing the function once you found the function. We could find the function, but now we need to generate data that helps to learn this task. And so ourmethod was simple. We take a correct code base, insert a bug, and then our task will be we describe the bug in a GitHub issue, just like in Sweetbench, and give the altered code base with the bug and also provide a search trace from the first step, from the first subtask. Together, the model could then use a the search context, which basically already points to the correct function, and then learn to edit the function to fix the bug. Around this time Sway Smith was released, a paper by a good friend of the press, and it used the same method as we did. The only difference was that they used unit tests to verify that the generated bugs actually break things. We didn't do that because we didn't have the resources to do that. We were thinking about doing this but it was much easier to just generate from a model and do not care so much about the infrastructure that we needed to create in order to verify tests and see if a bug was a real bug or not. Despite our incorrect or unverified bugs we got pretty good results. But what we also saw was that while we only need 500 examples to get such performance close to frontier model performance, with editing was quite different. Editing requires precise updates related to the bug. Because of this needed precision we actually need quite a few samples to do well. So we needed to generate a lot of different bugs with a lot of different samples. This procedure was still relatively efficient because once the search trace was generated, we could just append it and then generate the bug. But from this point, we realized, while creating search models very efficient, the second step editing model is basically just as computationally expensive as combining both steps into an end-to-end problems. But if you just want to do well on SweepEng, the end-to-end approach where we just create a bug is much more efficient. That reduced our method to something similar than Sweeney Smith, verified generation. Because it was so expensive to generate a lot of data, we quickly settled we can get pretty good data by not verifying it and not running tests. That reduce the cost of and running tests which can consume quite a bit of time in CPU cycles which time GLM 4.5 and then soon GLM 4.6 was released which which give very strong which complicates things further. The other thing that we wanted to do is find a sheet model to generate data from. They're very powerful models at that time, GLM 4.5, and then soon GLM 4.6 was released,which give very strong results on SweetBenge and are open weight, but the full GLM 4.5 model is just too big. You cannot deploy it efficiently if you have just 32 GPUs and experimentation becomes infeasible. When we use the GLM 4.5 Air model, we quickly realize that this is kind of the sweet spot. After testing several models, we saw that it gives strong speedbench performance at relatively low cost if you deploy it on a couple of GPUs. So with this we had low complexity in terms of verification and efficient generation by using the right model. This made things pretty cost-effective and once you're cost-effective you have a double win. First you can use your resources more efficiently. But second, you make faster progress on your problem, which often up. But scaling laws, main purpose is to make experimentation more efficient. So you can make So with this diversion, we rapidly made progress because our method is a sheep. the only thing that we do is look at the final patch generated and compared to with the patch thatwe mutated it from when we created the bug and then we just look for partial line by line overlap with this we have soft verification where for example if the model generates a patch that overlaps with 50 of the real patch that when we're created through the mutation of the correct code base being mutated into the bug then we would accept it as a training example once it reaches a certain threshold say 50 of the lines are overlapping the soft verification helped us with very fast data generation that's very cheap but that didn't stop us from further improving efficiency. One idea was to get closer and closer to real GitHub issues. And so what we opted for was we generate a bug, we solve the bug, then we create a new GitHub issue, and then we solve the bug again. This is much more efficient because generating the bugcan be quite complex. And so instead of using flops on both the bug generation and the rollout to solve the bug, we have two rollouts and one bug generation because creation of the GitHub issue is trivial once you have the trajectory and the initial bug. This means about 50%. But once we did this, we realized a couple of things. First, the model often But once we did this, we realized a couple of things. and vague enough that different solutions might be acceptable. For example, a bug might be created because of function reuse and the model might guess and say, I think this bug is related to some problems like refactoring than a bug fix and so that means we have refactoring traces even if we ask for a bug fix. Once we realize that this is widespread we embrace this vagueness and that led to a very unintuitive procedure. We take a correct code base and then so this led us to a procedure that is much cheaper complex and expensive, complex in terms of analyzing the code base to generate a bug that makes sense. For example, look at a function and its sub-functions and say that create a bug in a sub-function and then create an instruction that says there's a bug downstream of a particular function. you can't reveal to the model the direct location of a bug, but a bug should somewhat be related to a search query. So you need to give the model an initial point of entering to the code base without giving away where the bug is. This is complex and is expensive because it requires analysis and then very targeted bug generation with careful prompting, increases cost. So instead, by embracing vagueness, we use this following procedure thatis very unintuitive. We take a correct code base, then select a random function, and then tell the model there's And so because of this vagueness, the model goes down this function and then just looks and then tell the model there's a bug downstream from that function. and then just looks at the code and thinks up a bug that actually doesn't exist. shouldn't be passed. Or that assertion was missing that captured a case which shouldn't be passed into a function. So while the instruction is to create a bug, the model created all kinds of things that actually make good sense. To make generation sheep, we kept the rest of the pipeline. That means we create a bug from a correct code base in a simple prompt that starts with a random function. Then from the trace that we generated, we create a GitHub issue. And what we then do is do another rollout where the beginning is not a random function, but the GitHub issue itself. That helps us to generate lots of training data from just a random functional code base. We still look atsoftware verification where we look at the overlap of the first solution and where we compare a solution of the GitHub issue for the soft verification. Two trajectories are needed, and we create them succession, which makes everything flop efficient and very simple. All you need is a code base with functions and a model that can generate trajectories through two calls. This means we have a higher data limit. Usually data generation pipelines are limited by how many bugs you can create that make sense. If you verify bugs for correct bugs that actually make tests fail, this further limits the amount of data you cangenerate. And remember you need to generate a bug but also a starting point that doesn't give away where the exact bug is. In our case, we don't have these limitations. If a code base has a thousand functions, we can generate a thousand trajectories. To further amplify how much data we can generate, we look at papers that provide analyses of bugs commonly found in engineering. So from that we get a distribution of bug types. and overall 51 bug types. we can generate 51,000 trajectories very easily, very cheaply. Furthermore, compared to Sui Smith, we are about five times cheaper. So for each function, we can generate 51 different bugs. That means if a code base has 1,000 functions, we can generate 51,000 trajectories very easily, very cheaply. This means we are almost not data limited. Furthermore, compared to Svesmith, we are about five times cheaper. The trajectory that we generate costs about two cents. and just needs 36 GPU seconds to be generated. Our data quality is also slightly higher, mostly because we use a better model, than Sui Smith. Furthermore, our model is six times more efficient in terms of cost per token. A large cost of SuiBench is the bug generation and the validation of that bug. Since we don't verify and we actually don't generate a bug, we just assume that there's a bug in a correct code base and needs to be fixed, we skip this cost. Overall, compared to other methods like Sweeney Smith, we are 60 times cheaper per trajectory. Compared to reinforcement learning like SkyRL, we are 90 times, 90 times more. efficient and cheaper. With all of this, it means we can generate massive amounts of data for any code base. That helps us to explore where are the limits of scaling. All of this leads to Sarah, soft verified efficient repository agents. Next subsection, soft verified efficient repository agents. Now combining all of this, we try to tackle the holy grail of coding agents. And this holy but what many people noticed is once you work on programming languages that have less and this holy grail is private code basespecialization. the models, the closed source models don't do as well. Now the holy grail is to take a model and actually specialize it on private data. This is exactly what we're doing and it works really well. With our method, we generate massive amounts of data for a particular repository. It's easy to generate 7,000 trajectories, which just takes one GPU hour if you run on eight eight one hundredths then we train on those trajectories and what we get is a 32 billion model that is as good as the teacher model in this case as good as glm 4.5 air the model that we generate the data from it seems that this also works for other teacher models which means you can compress frontier level performance specialized to prior private data into a tiny model that you can easily deploy what we actually find is that with enough data we exceed teacher model performance and this makes sense because we generate data from private repositories that the model hasn't seen that the teacher hasn't seen the student actually can exceed the teacher on this due to the specialized data It goes without saying that this is a massive result. That helps companies to quickly specialize a small model to frontier level performance, or even exceed it, on their private data. customize coding agents, broader impacts. The next section, broader impacts. We believe that with our methods, we step into a new era of coding agents. For one, private specialization, coding agent specialization on private data allows companies to take advantage of coding agents. The second, because our method is so cheap, this opens up coding research for all kinds of researchers. You do not need large teams to maintain and create reinforcement learning setups that are highly complicated. You don't need thousands and thousands of dollars to just replicate a baseline and more and more thousandsand thousands of dollars to actually do research. Our baseline just costs $200 to create. This means any lab that has a couple of GPUs will be able to do coding agent research. I wouldn't be surprised if academic labs reproduce frontier-level coding performance within the end of 2026. We are already seeing that open weight models approach frontier-level coding. GLM 4.7 is very powerful. I believe if we take open weight-based models, we will quickly have procedures with which we can replicate frontier coding. model performance. But now, with our method and future methods, you will be able to specialize it to your private data. There's a real promise that open source models will exceed frontier level performance because you don't need to expose your private data. You can keep it and just create your own model and deploy it.
