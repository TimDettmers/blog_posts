# Use Agents or Be Left Behind? A Guide on How to Use Agents to Automate Your Own Work

The excitement about Claude Code and agents has exploded. There is a lot of FOMO about agents, and the reality is that some of it is real and some of it is not. I have been using agents—primarily Claude Code—for eight months to automate my own work. This blog post is about the perspectives I have gained in those eight months, best practices, and also an expansion of what you can do with coding agents.

Most of the tasks that I use coding agents for are actually writing. I also build tools, but it is mostly writing. As a professor, I do not do that much coding anymore, and so this blog post offers a very personal view that is often missed. If you look at software engineering discourse, it is mostly just FOMO that is driving the conversation. Software engineering is a very particular task that is not related at all to how agents will be used for most tasks.

This blog post will go into what is hype and what is not, why you need to use agents or be left behind, how to think about automating your own job, what are helpful concepts, and what are unhelpful concepts. I will also give many examples of where agents work well and where they do not.

Just to give you a hint of how powerful agents have been for me: usually the first year as a professor is very stressful and involves a lot of work. For me, it felt very easy. I had quite a bit of luck here and there, but I feel like the use of agents is a good reason why things have been easy for me and hard for others.

## What Is FOMO and What Is Real

### The Software Engineering Hype

What you see on Twitter is mostly about software engineering. While agents in software engineering are real and you need to use agents for software engineering, it is very unlike many other problems.

In software engineering, you often have many parallel problems that you work on: bugs, new features, quality control and refactoring, GitHub discussions and reviews. All of these tasks are independent and can be parallelized. Usually a bug does not depend on a feature. Reviews are on top of fixed bugs or added features, so they do not interfere with other bugs and features that come after. Features are not typically developed on top of each other very rapidly.

While AI agents are very strong for creating custom tooling, complex code bases make it difficult to work with agents. But complex code bases also offer the opportunity for more parallel work. You can always add a little bit here, a little bit there. You can always fix a bug here and a bug there. The larger the code base, the more independent work can be done, and the more parallel sessions are useful.

The concept of parallel sessions is not useful for most tasks. It can still be useful to think about parallel sessions in terms of: how can I parallelize my tasks? Agents need quite a bit of time to generate output. You can think about how to restructure how you work so that most of your time is spent inputting information into agents rather than waiting for them to complete work. This is not quite easy. You need to context switch quite a bit, and it can be very inefficient in many cases because context switching reduces the quality of your work. But it can also be real and effective.

### The Real Gains of Agents

More than 90% of code and text should be written by agents. You need to do so, or you will be left behind.

This statement might seem as controversial and as much FOMO-inducing as the software engineering story of parallel sessions and complex custom workflows. But that is just reality. A lot of the work that we are doing can be prepared by coding agents.

When I talk with people about this concept of almost everything being generated by AI, they find it a ridiculous statement. How can generic boilerplate generation of agents replace the intricate details of code and text that is distinct from any other person?

I think this is a delusion.

## Why AI-Generated Content Is Personal, Not Generic

AI-generated content is personal content. When I explore a concept with Claude, the output is not generic. It is shaped entirely by my thinking, my style of conversation. I really like connections between fields, and so many topics I am curious about and explore are highly personal and unique traces of my thinking—and with that, my taste.

For example, I once started a conversation about jihad, the concept in Islamic theology that is often misunderstood, about doing the right thing rather than the easy thing. From there, I ended up connecting it to Krishna's advice to Arjuna about surrendering the fruits of action (karma yoga), to Lutheran grace that emerges at the height of struggle, to Taoist Wu Wei where struggle disappears through letting go and letting your nature take over, to Beowulf and naked will against overwhelming odds (the fight with Grendel).

None of this exists in any textbook or on the internet. It is a fingerprint, a very personal fingerprint. If you were to read the details of these conversations, you would know parts of me intimately—who I am and why I am that way. You would know me to a degree that is usually only reserved for close friends and your partner.

Someone thinking that AI-generated content is impersonal and generic is deeply mistaken. The concept of soulless AI generation is an artifact of less powerful AI, or the mistake of seeing your own generations as what AI can do rather than recognizing it as a skill issue.

Almost all code and text will be AI-generated in the future. That does not mean that it is generic and impersonal.

And just to make a point: you might have not noticed this, but this entire blog post is almost entirely AI-generated.

## Helpful Concepts to Automate Your Own Work

### Basic Thinking About Automation

Before I worked in AI, I worked in the automation industry in Germany. I was automating SCADA systems, which integrate data from machines and databases to enable the control of workflows via data and monitoring. My three years in the automation industry applies to automating your own work with agents as well.

The first important question is: when should you automate versus when should you not? While people always think about automation in terms of full automation, this is almost never the case in practice. You always have a degree of automation.

How you can think about useful automation is this: if you take your current degree of automation and increase it by a new technology, then you increase by a certain percentage that makes you more effective. If a task takes 10 hours and you improve the degree of automation by 10%, then it takes 9 hours. It is a simple calculus: how often do you do the task, and how long do you need to automate this task in such a way that the degree of automation improves by 10%?

If this calculus leads to a result where the cost of automating something is higher than the gain, then the problem is not primed for automation. It is better to not automate it. There are many tasks that should not be automated because it is not effective.

Additionally, as your workflow changes, it adds a little bit of overhead. For example, you might save 30 seconds, but if your agent needs 30 seconds to generate that automation, then the effectiveness is 0%. The degree of automation improves by 0%.

### The Method: Process Optimization

In automation, if you want to improve automation, a very basic method is to be on the factory floor with a stopwatch and then look at and study what workers are doing. You take time for each step they are doing and how they are handing off work to another person, when they are waiting for previous work to complete so they can start their work, and how many resources they need to do their work.

If you have all these components, you can construct a workflow—a process that reflects the current state of how you work. Based on this, you can think about how to optimize a process. This thinking is extremely important for automating your own work. You have to think about how you work on a particular problem and how you can change your work with agents. Sometimes you find that the process cannot be optimized so much with agents.

For example, if I take one minute to read an email and 30 seconds to reply, then it takes 1 minute 30 seconds to complete an email. Now, if I use an agent to help me with my emails, then I need to guide it to process my emails. Then I need to read that content to decide how it should create drafts or answer emails so that I can then edit those drafts to complete my emails. But once you do this exercise, you will realize that by using agents, you just shift the process, but you still need to read content.

There are certain emails that are easy to automate. There are others that are not. It depends on your process and your underlying inputs to see if using agents and changing your process or workflow can actually lead to productivity.

As I will discuss in the sections below, I failed to automate emailing. Doing it fully manually with an optimized manual process is always faster and more precise than using agents. I worked on agents for emails for two months. I could not get it working. I do not think it is a problem that can work for a professor. It might work for other professionals, but I think it is very difficult to apply to certain problems.

The exact thinking here is one that you should adopt. Reading an email or reading AI-generated content has a cost. You need to include that cost to understand if you can benefit from automation.

### Short-Term Versus Long-Term Automation

The short-term perspective on automation is mostly the perspective I just gave. You look at the underlying processes and the degree of automation, then think about how long you will need to automate that work and how much you increase the degree of automation. It is a simple calculus. Once you do this calculus, you know if an automation might be useful. This is classic automation, how it is done in Germany and other countries. This is very cost-effective and optimal in the short term.

However, it is very short-sighted because it does not consider long-term consequences.

The long-term view is a Shenzhen-style perspective. It is not about making any automation useful in the short term. It is making an automation useful in the long run by gathering knowledge that improves automation in the long run.

With the thinking above, you need to add: even if the degree of automation is not worth it in the short term, the skills that I build and the tools that I develop—will they make future automation effective that was previously ineffective? Does the additional knowledge help me with future automation?

This is exactly what led from Shenzhen-style scrappy factories that are messy (every factory is messy, but there is a degree) to highly structured, dark factories that are fully automated. Chinese automation is far superior to Western automation, not because of the scale, but because the long-term view of automation led to a higher quality and degree of automation.

This is an important concept. You need to optimize both short-term and long-term perspectives to effectively automate your own job. You see the importance of this by looking at the current state of automation. Europe is struggling. The US is struggling in many segments because they did not build the long-term skill set initially—the profitable or unprofitable efforts that help them build the automation muscles to tackle more challenging problems and increase the degree of automation over time.

In other words, using agents and failing at automating a task effectively is important. You need to gain skills to improve future automation, and that means sometimes trying to automate things that you know you will not be able to automate.

## Why Automating Your Job Is Good for You

Software engineers are not replaced. They just level up. The current hiring crisis is made by COVID and financial dynamics much more than by AI. Software engineers are much more effective at building software more rapidly, and the value of software on its own has not decreased by much. An engineer that uses agents like a hot knife slicing through butter is actually more valuable because they can produce more software that still has significant value.

A common theme is that this is the current state, but software engineers will become obsolete very soon. I have a lot of friends at frontier labs that had the same view about nine months ago, but it has broadly changed because they see that it is difficult to automate their own work and that as they use these tools, new problems open up.

Even if an agent can do everything, they cannot do everything at the same time. If you have a limited amount of GPUs, you want to direct agents to tasks that are useful for you so they can generate value where you need it. While even that can be partially automated, once your existence is at stake, you probably want to direct what agents do yourself—at least specify the problem and solution you want.

I think it will be a long time until you use an agent to manage your retirement savings by analyzing the stock market and optimizing it. But what is more reasonable is you build a system where you tell an agent what risk you are happy to accept and how to optimize this risk mathematically through hedging, so that you might manage your retirement fund in a way that has a trade-off between the potential for a strong upside versus risk over time. It would be stupid to fully trust an agent if you do not know the parameters that are important for you and how the agent chooses those parameters.

If resources are limited, you want to decide how those resources are used rather than fully trusting an agent. And if this is true, then directing agents will remain a problem, even if agents can do everything, because agents cannot do everything at once, because resources are finite.

Long story short, because of this resource problem there will always be work where your personal preferences, decisions, and taste will be needed to do work—even if most of the work, 90% of the work, happens through AI. From software engineering we already see that this work changes, but it will not eliminate our jobs. We also see it will impact jobs in terms of: if you do not know how to use agents, you will not have a good job or be able to find a job. Agent use is now an essential skill that you need to develop.

## My Personal Experience with Automating My Own Work

### Where Agents Work Well

#### Custom Tooling and Pipelines

What is most common on Twitter as examples of successful agent use is when people create a tool that is useful for them. These can be small extensions that help your everyday life—just vibe coding something that you always wanted and that is simple, but nobody provided.

While this is a very simplistic way of using agents, it has its importance. This is a problem where agents work really well, and they require very little skill to be used correctly.

For example, I built tools that help me write this blog post. I built tools that help me work with agents. One of the most important tools is a voice tool, which helps me to quickly interact with agents, particularly if I have parallel sessions. A voice tool also helps me because I have carpal tunnel in both my hands. Typing can be painful. I have a very custom keyboard layout and way of working with the keyboard that reduces pain to almost zero, but still it is much more comfortable to just use my voice. And it is not only comfortable, it is faster.

A main advantage is that with voice you can inspect outputs and use your keyboard and mouse while narrating. This is extremely powerful. A key tool that everybody should develop is their own voice tool to use AI in this way where they can do work while narrating. It is very important.

Other tools I build are mostly for my students. For example, it was recently revealed that quite a bit of Claude Code use is actually by exploiting it as an API. This means you use a Claude Code endpoint just as an API to use it in other work. That gives API access. My students use it, and when I asked them if they need any GPUs, they said no—they just generate with the API that we created. That has been a very useful tool.

#### Connected Papers Replication Tool

Another tool I built was to solve the problem of finding related work. The most useful tool I have ever used for finding related work is Connected Papers. It was free at some time, but then got commercial and now requires a subscription. I need something like this at the beginning of a project and when writing the related work section of a paper. I did not want to pay for the subscription, and I wanted my students to be able to access it. So I just replicated the entire software system.

This was one of the things that probably was not effective for automation in the short term—I could have just paid for Connected Papers. But it gave me an overview of what I can do in the long term: what tools can I build, what is too ambitious, what is less ambitious, how can I be more effective when creating complex tools that integrate things.

The Connected Papers tool that I built uses the Semantic Scholar API to retrieve data. Then it builds statistics on the citation graph of papers to find papers that are very similar to what Connected Papers finds. The key insight I had is how Connected Papers works: it finds papers that are in conversation. They are often indirectly related to another paper through a third paper that cites both of them. They create a chain, a loop of three papers, and this loop is distributed. If you have this loop across three-paper chains that create circles, you have a very good way of finding related papers. Of course, you want to weigh this by the amount of citations each paper has, particularly if you normalize over time. But this was the key algorithm that made the tool very useful.

The tools that I build are easy to use. The easiest thing is just to host them on the web. Similar to Connected Papers, everybody can just access it and use it—they do not need to run Python commands, have API keys, and those kinds of things. But I have not created a pipeline yet that publishes tools on the web, makes them broadly accessible, while also ensuring that not everybody has access so they drain my resources or make me run into rate limits if I use an API key like Semantic Scholar.

You see that even creating tools can have its own complexity. While not highly successful in the short term, certain automation gives you a lot of skills that are useful for long-term optimization. I would encourage you to spend some time on futile projects just to learn how to improve automation in the future.

#### Other Tooling

Other tooling that is much more straightforward includes infrastructure for Slurm and common infrastructure to analyze experiments. I believe Weights and Biases is total crap and actually harms research problems.

A tool I have not developed yet but which my colleagues (other professors) have mentioned is a review system where students can get feedback on ideas or papers by querying an agent or an agentic pipeline that mimics how an academic advisor would give feedback or review a draft for an idea.

While not all of these tools might be useful, and some are more like distractions, it is clear that with the right pipelines, workflow, and tools, productivity for students can be increased dramatically and can be driven by an advisor.

Similarly, a technical manager can probably develop tools and guide a team. Even if agents cannot do any work, you need to figure out what work you actually want to do and how you want to build on each other's work as a team. Agents can work independently, but it might not be very useful if you pull on different ends of a problem, if your team is not coordinated.

All of these highlight where tools fail, where tools can be useful, and where tools might not be useful but give you the skills to improve tools in the future.

### Writing Tasks with Clear Structure

#### Blog Posts

You might have guessed it already: this blog post is AI-generated. More than 95% of the text comes from an AI model. I did not even type prompts. Most of it was just me rambling into a microphone while doing other things, then transcribing that voice into text, shaping it into a blog post, then doing a style transfer to shape it into a blog post in my voice, and then adding some small snippets that have character.

The last piece is a cherry on top that converts a blog post that already has my style into a blog post that I truly own. But these edits make less than 5%, maybe even 2% of the overall blog post.

While I am still experimenting with blog posts, this pipeline allows me to write blog posts much more quickly—and blog posts that are much more current. A blog post like this would have taken me days in the past. Now it takes about one and a half hours: one hour to speak content into a microphone, 10 minutes for my agentic workflow, and then 20 minutes to review and edit the passages. It is very fast, and using agents you notice that the quality is pretty good. If you look at my last blog post about why AGI will not happen, which by the way I will discuss in an upcoming podcast—you can see this in action.

#### Grant Proposals

Grant proposals are a major time sink as an academic. A CMU student costs $150,000, and I need to find that money by writing grant proposals. A lot of grant proposals are rejected, so you have to write lots of them. This can be very time-consuming, so naturally I tried to automate this problem.

It is interesting because while you might think that the blog post approach I described in the previous section should work, it actually does not work that well. Grant proposals need to have a particular structure, and even small deviations read poorly. Good design is familiar design, and good proposals are familiar proposals.

Just like a good abstract—for example, an abstract in Nature has almost always the same structure, sentence by sentence, the same for every paper. That makes it easy to read abstracts because you know where to find information. I am dyslexic, and reading is very slow for me, but I can actually read papers relatively quickly because they have common structure. I can skip sections, skip to particular phrases, and I know here begins an interesting part. If the introduction says "in this paper" or "here," then I know now the contributions begin and I need to read that.

Just like papers, grant proposals are highly structured. A free-flowing, talkative approach does not work out of the box, but it can be made to work by introducing an abstraction pattern.

This abstraction pattern works as follows: you create sentence-by-sentence extractions of what the grant proposal content should be. For example, for an abstract, the common sentence-by-sentence abstraction is:
- The first sentence is about the general field and the subfield
- The second sentence mentions the problem, why it is important, and why it has not been solved
- The third sentence states your contributions and your main results
- The fourth sentence explains your main method
- Then, depending on taste, you expand on this method for several sentences or keep it brief
- Finally, you state the impact and broad implications

If you have an AI model, you can apply this process very easily. What I told an agent was to create a pipeline which gives me particular questions about what content is needed to fill in all these placeholders—these abstract placeholders. Then I smooth it over by doing style transfer using particular proposals that I have written and like. The next step is very similar to the blog post: I just edit and revise.

With this, I can create a four-page grant proposal in about an hour or an hour and a half—very similar to a blog post.

#### Writing Research Papers

This is ongoing work that I will probably complete as my next milestone to improve automation in my own work and in my students' work.

### Meta Reviews

Machine learning conferences are notorious for bad reviewing. The reviewing system is broken. There have been studies on ICLR and NeurIPS with some clear results: reviewing does not work. Reviewing cannot identify the really worst papers and the best papers, but in between it is a coin flip.

The finding from these studies is also that reviewing quality is not related to knowledge but related to effort. Undergrad students have much higher quality reviews than PhD students or professors because they have more time and take it more seriously. For PhD students and professors, it is a chore. As such, using agents to create reviews or meta reviews is important to improve reviews and the entire review process.

Looking at that reality, using agents becomes very straightforward. And there is no concern at all about doing pool meta review.

There are two different philosophies about being an area chair. One is you bring your own opinions with you and overwrite decisions. The other is to just follow what the reviewers said. I believe the second is more intellectually honest. While I have lots of expertise and sometimes I will overwrite reviews, I have not had the depth to read a paper thoroughly, and certain concerns might be valid. In the end, a good paper is not a paper that I like, but a paper that is useful for the research community as a whole. As such, I have a philosophy of siding with the reviewers.

What I built is a system to analyze the discussions, the points where reviewers disagree, give summaries of papers, summarize which papers are borderline, and identify which papers are clear rejects or accepts. The clear accepts or rejects have high variability with very high scores and very low scores. These reviews can be processed quickly—you can understand why people have a certain view.

What is more subtle is tracking changes in the discussion. With the rebuttal, even if the score is not increased (which is very common because people do not have time), the rebuttal might contain information that could change the outcome of acceptance or rejection of the paper.

From all of this discussion about borderline cases, you can easily draft the first meta review. If it looks strange, you can ask the agent to explain, provide more detail, or provide evidence. It is a very interactive way of reviewing and actually mirrors what I would do if I were an area chair without AI agents. For example, I would look for disagreements of views that reviewers have, why some reviewers have high or low scores, the quality of reviews, and if people write in a biased way that might compromise their review.

All these things can be done by an agent, and they can be done faster and probably more precisely. Understanding a subtle argument of a paper I have not seen before, between reviewers that have different perspectives and maybe read the paper in a different way—this is hard if it is 5 PM and I have already had eight meetings that day and I am just tired. But if I do it with my voice tool and my meta review agent system, this allows me to write high-quality meta reviews.

AI-generated content is highly personal if you do it right. This also goes for reviews. An AI-generated review is your review if you do it right. There is a danger of generic reviews, but at the current state of extremely noisy reviews, this is not the most relevant point. I think we really do a disservice to the research community if we do not use agents for reviewing.

### Research Coding

As a professor, I am not that actively involved in research anymore. However, I will do active research in the summer, and I aim to create a class or course on how to use agents in practice for your research. For that, I will study how to effectively work on research problems.

A key insight for research coding is the following: once computers became available, in all scientific disciplines, there was a selection for problems that are well shaped to be pursued with computers. It is unthinkable today to work in a field on a problem which cannot use computers if there are many problems that can use computers.

Similarly, in the future it will be unthinkable to work on a problem where agents do not work well. There are so many problems out there in science, and you cannot solve all of them. Focus on the problems that have high impact and are ripe. "Ripe" often means it can be done with the resources that you have, with the time that you have. Agents change the equation, and doing research where agents do not do well will become outdated and obsolete.

This concept and others will be part of the course that I teach. I will develop this course over the next nine months and will publish lectures on YouTube. It will be experimental initially, but then I will structure it into an openly accessible course that anybody can do.

### Where Agents Fail

#### Email Automation

I alluded previously that I tried to automate emails for a long time. Over two months, I worked quite a bit on automating emails—for one because I do not like emails, and also because it is now a major part of my work. Every day I have to reply to countless emails, or I have to write emails to do my work.

I wanted to build a system that helps me manage, prioritize, and draft emails. Probably for most people, the problem is similar: knowing which emails they have replied to, what needs work, what still needs to be done, what is important, and what can be ignored or replied to later. Doing this manually is very simple and fast. I can often look at the title and immediately sort it into a category, or I can open an email and see: do I need to reply now, or is tomorrow fine?

My initial system was very focused on features. If you think about these problems, you think it would be easy to categorize which emails you need to reply to now, which you can reply to later. You can also easily categorize emails. I get a lot of requests from students that want to work with me—that is one category. There are important emails related to things I care about or that are important for my lab, like work with funding partners.

But here is the issue: you need to look at those categories. If you create drafts, you need to look at the drafts. In Gmail, it is easy because you know the interface, and it is very simple to navigate and categorize things yourself. If an AI does that, it is similar but automated. But you still need to look at what you actually need to do. And this costs time, just like how it costs time to read the title or skim an email to categorize it.

Here, the process optimization view kicks in. If I can categorize an email within five seconds, that is pretty fast. An AI agent needs to beat that in five seconds and be more precise than I am for it to actually be useful. While the reading and categorization can happen in the background, with an AI-generated categorization I actually need to still skim an email and skim the draft that the AI generated. That might take 10 to 30 seconds, and that might already be slower than just reading the title and deciding I will not reply to this email right now or never.

Something very similar goes with drafting emails. The problem is that drafts need to be very precise. What I engineered was a system that provides enough context so an AI can draft well, but the editing part—understanding what was going on and if it was precise enough—often took slightly more time than just replying to the email. Because I cannot trust an AI 100%, I actually need to read the email, read the draft, and then edit the draft. In a workflow where you do all of this manually, it is often faster, even for problems where automation works.

There are structured problems that work better. For example, I have students that want to work with me, and my response is often the same. If I can only take one student next semester but no student right now, I will respond along the lines of: thanking the person, telling them that I currently do not have openings, but I have an opening next semester and they should get back in touch with me then. An AI can, given the right context, generate all of this very easily.

I even went a step further and automatically downloaded the CVs, parsed them, and had the AI create a spreadsheet with key information. You can actually review quite quickly, within about 15 to 20 seconds per candidate. You know if a candidate is aligned with your research and has good research experience, or if they lack the right experience or are misaligned with your research.

But then it was difficult to parse and analyze the results—to see how correct the AI was, if things were missing, how precisely the research aligns with mine, how flexible students are in terms of research interests, a certain vibe that they present in their CVs and emails. All of these can be evaluated quite quickly. For example, the vibe of an email can be evaluated within one second or two seconds. It is a very rich signal that can help with selection. So manual reviewing is often faster. For example, I spent yesterday about one hour to review 50 candidates—a bit more than one minute per candidate.

Despite all these edge cases, I did not want to give up. For one, I really do not like emails. But the second part is that for me it was a challenge: can I automate this task? And if not, I will gain valuable experience that might be very important in the future.

So I made a second attempt. I knew about the process. I knew about the importance of interfaces and how to structure information. Since I am an avid user of Vim, both in operating my interfaces and the operating system, I know how to interact with information efficiently.

This was a very long process because co-designing all these factors is not easy. There is just a certain beauty in the ease of flow. So it was a co-design of functionality, agents, and user interface—a little bit slow. It improved day by day, but at some point I asked myself: is maybe Gmail, if I use it in the right way, faster? I saw the improvement I had each day plateauing.

So I compared time spent on emails between the tool I created—optimized with user interface, interaction, content, and agents—and just using Gmail. What I found is that just using Gmail is faster. I could not get any degree of automation improvement by using agents for emails. But it was an important skill to develop.

## Conclusion

This concludes this blog post. I hope these concepts have been useful to help you think about how you can use agents, where agents work well, and what is hype and what is not.
